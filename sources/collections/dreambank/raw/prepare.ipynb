{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635b4d94",
   "metadata": {},
   "source": [
    "# DreamBank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b8a54",
   "metadata": {},
   "source": [
    "This notebook scrapes raw HTML dream reports (and their associated metadata) from [DreamBank](https://dreambank.net) and compresses them into an archive file for uploading to Zenodo.\n",
    "\n",
    "The final archive includes an HTML file for the [grid page](https://dreambank.net/grid.cgi) and then 3 HTML files for each dataset. All dataset IDs are identified from the [grid page](https://dreambank.net/grid.cgi). Taking the Alta dataset as an example, the 3 HTML files will come from the [Alta info page](https://dreambank.net/more_info.cgi?alta), [Alta more info page](https://dreambank.net/more_info.cgi?series=alta&further=1), and [all Alta dreams page](https://dreambank.net/random_sample.cgi?series=alta) files per dataset.\n",
    "\n",
    "The files are written as raw binary content in order to preserve the data in the rawest form possible. Dream reports and metadata are extracted from the raw HTML content in [prepare.ipynb](../prepare.ipynb), and datasets are curated from that in [krank](https://github.com/remrama/krank). See the [krank documentation](https://remrama.github.io/krank) for more accessible access points.\n",
    "\n",
    "For output file size, checksums, and date of processing, see printout at the end of this notebook.\n",
    "\n",
    "* Source: [dreambank.net](https://dreambank.net)\n",
    "* Output: [Zenodo](https://doi.org/10.5281/zenodo.18131749)\n",
    "\n",
    "If you use any of this data for publication, cite the original DreamBank paper:\n",
    "\n",
    "> Domhoff, G. W., & Schneider, A. (2008). Studying dream content using the archive and search engine on DreamBank.net. _Consciousness and Cognition_, 17(4), 1238-1247. doi:[10.1016/j.concog.2008.06.010](https://doi.org/10.1016/j.concog.2008.06.010)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5466e2ea",
   "metadata": {},
   "source": [
    "### Output file tree\n",
    "\n",
    "```shell\n",
    "dreambank.tar.xz\n",
    "|\n",
    "|--/grid.html                   # https://dreambank.net/grid.cgi\n",
    "|\n",
    "|--/<dataset_id>/dreams.html    # https://dreambank.net/random_sample.cgi?series=<dataset_id>\n",
    "|--/<dataset_id>/info.html      # https://dreambank.net/more_info.cgi?series=<dataset_id>\n",
    "|--/<dataset_id>/moreinfo.html  # https://dreambank.net/more_info.cgi?series=<dataset_id>&further=1\n",
    "|\n",
    "|--/<dataset_id>/dreams.html\n",
    "|--/<dataset_id>/info.html\n",
    "|--/<dataset_id>/moreinfo.html\n",
    "|\n",
    "|--/<dataset_id>/dreams.html\n",
    "|[...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1525e8a",
   "metadata": {},
   "source": [
    "### Related files\n",
    "\n",
    "1. This notebook downloads the raw HTML files from [DreamBank](https://dreambank.net).\n",
    "2. The file is uploaded to a [Zenodo archive](https://doi.org/10.5281/zenodo.18131749).\n",
    "3. The next [prepare.ipynb](../prepare.ipynb) notebook parses the HTML into two tabular CSV files, `datasets.csv` and `dreams.csv`.\n",
    "4. Then subdirectories in the [sources](../../../../) folder have individual notebooks that use the tabular files to compile individual corpora into a krank-ready corpus and are uploaded individually to Zenodo archives. These are not necessarily the same datasets that DreamBank provides, but custom groupings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59330f7c",
   "metadata": {},
   "source": [
    "### Related projects\n",
    "\n",
    "* [mattbierner/DreamScrape](https://github.com/mattbierner/DreamScrape)\n",
    "* [josauder/dreambank_visualized](https://github.com/josauder/dreambank_visualized)\n",
    "* [MigBap/dreambank](https://github.com/MigBap/dreambank)\n",
    "* [jjcordes/Dreambank](https://github.com/jjcordes/Dreambank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f5ad88",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e48541a",
   "metadata": {},
   "source": [
    "Load all necessary Python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71123a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659da348",
   "metadata": {},
   "source": [
    "Set constant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2ca1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BASENAME = \"./output/dreambank\"\n",
    "OUTPUT_COMPRESSION = \"xztar\"\n",
    "GRID_URL = \"https://dreambank.net/grid.cgi\"\n",
    "GRID_FNAME = \"grid.html\"\n",
    "MIN_WAIT_TIME = 5  # seconds\n",
    "MAX_WAIT_TIME = 9  # seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53323ef8",
   "metadata": {},
   "source": [
    "Remove existing file(s) from previous runs and create necessary directories. The temporary directory will be used to store the HTML files that will get compressed into a single output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98f1f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.dirname(OUTPUT_BASENAME)\n",
    "if os.path.isdir(output_dir):\n",
    "    shutil.rmtree(output_dir)\n",
    "os.mkdir(output_dir)\n",
    "temp_dir = tempfile.TemporaryDirectory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a76351",
   "metadata": {},
   "source": [
    "Create a function for compiling DreamBank-specific dataset URLs. URLs are dataset-dependent and can be easily generated by inserting the name of the dataset. There are 3 pages available for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2b247a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose_url(dataset: str, component: str) -> str:\n",
    "    \"\"\"Compose DreamBank URL for given dataset and component.\"\"\"\n",
    "    assert component in {\"dreams\", \"info\", \"moreinfo\"}\n",
    "    if component == \"dreams\":\n",
    "        return f\"https://dreambank.net/random_sample.cgi?series={dataset}\"\n",
    "    elif component == \"info\":\n",
    "        return f\"https://dreambank.net/more_info.cgi?series={dataset}\"\n",
    "    elif component == \"moreinfo\":\n",
    "        return f\"https://dreambank.net/more_info.cgi?series={dataset}&further=1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021b6db1",
   "metadata": {},
   "source": [
    "Create a function for printing ISO-formatted timestamps later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ad22e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(unix_timestamp: float) -> str:\n",
    "    \"\"\"Convert unix timestamp to UTC-stamped ISO format.\"\"\"\n",
    "    dt = datetime.fromtimestamp(unix_timestamp, tz=timezone.utc)\n",
    "    timestamp = dt.isoformat(timespec=\"seconds\")\n",
    "    return timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784ab7b2",
   "metadata": {},
   "source": [
    "## Grid page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adfd4c9",
   "metadata": {},
   "source": [
    "The grid file is a DreamBank page that includes a table of all the datasets available in DreamBank and also longer text descriptions of them. This file is used to identify all the dataset IDs included in the current version of DreamBank and used for subsequent scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8d887",
   "metadata": {},
   "source": [
    "Download the grid page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31464eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(GRID_URL, headers={\"Accept-Encoding\": \"gzip, deflate\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab573af",
   "metadata": {},
   "source": [
    "Extract the dataset IDs from the [grid page](https://dreambank.net/grid.cgi) by finding all the checkbox elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66013790",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content, \"html.parser\", from_encoding=\"ISO-8859-1\")\n",
    "dataset_ids = set(x.get(\"value\") for x in soup.find_all(\"input\", type=\"checkbox\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eacc917",
   "metadata": {},
   "source": [
    "Write the grid content to an HTML file for inclusion in the final output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41e71ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(temp_dir.name, GRID_FNAME), \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79ca635",
   "metadata": {},
   "source": [
    "## Dream reports and metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70f7ef",
   "metadata": {},
   "source": [
    "Loop over all the dataset IDs from the grid page, downloading all 3 component pages for each dataset and writing each one to its own HTML file in the temporary directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2144ed48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving content (zurich-m.de): 100%|██████████| 96/96 [37:05<00:00, 23.18s/it]        \n"
     ]
    }
   ],
   "source": [
    "for dataset_id in (pbar := tqdm(sorted(dataset_ids))):\n",
    "    pbar.set_description(f\"Retrieving content ({dataset_id})\")\n",
    "    dataset_dir = os.path.join(temp_dir.name, dataset_id)\n",
    "    os.mkdir(dataset_dir)\n",
    "    for component in [\"dreams\", \"info\", \"moreinfo\"]:\n",
    "        url = compose_url(dataset_id, component)\n",
    "        fname = f\"{dataset_id}/{component}.html\"\n",
    "        response = requests.get(url, headers={\"Accept-Encoding\": \"gzip, deflate\"})\n",
    "        local_fname = os.path.join(temp_dir.name, fname)\n",
    "        with open(local_fname, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        wait_time = random.uniform(MIN_WAIT_TIME, MAX_WAIT_TIME)\n",
    "        time.sleep(wait_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33502dd4",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d70a3c",
   "metadata": {},
   "source": [
    "Zip the cache directory into a single compressed file and delete the temporary directory that held individual files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fcf9068",
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = shutil.make_archive(OUTPUT_BASENAME, OUTPUT_COMPRESSION, temp_dir.name)\n",
    "temp_dir.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db9e4fa",
   "metadata": {},
   "source": [
    "Print the output file details for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9bc3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      file: dreambank.tar.xz\n",
      "      size: 8.319932 MB\n",
      "       md5: eb83bcb0828f9c8c248a5052b2ffc798\n",
      "    sha256: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n",
      "   Created: 2026-01-05T17:09:52+00:00\n",
      "  Modified: 2026-01-05T17:09:52+00:00\n",
      "  Accessed: 2026-01-05T17:09:52+00:00\n",
      "       Now: 2026-01-05T17:09:52+00:00\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'file':>10}: {os.path.basename(outpath)}\")\n",
    "print(f\"{'size':>10}: {os.path.getsize(outpath) / 1e6} MB\")\n",
    "with open(outpath, \"rb\") as f:\n",
    "    print(f\"{'md5':>10}: {hashlib.md5(f.read()).hexdigest()}\")\n",
    "    print(f\"{'sha256':>10}: {hashlib.sha256(f.read()).hexdigest()}\")\n",
    "print(f\"{'Created':>10}: {format_timestamp(os.path.getatime(outpath))}\")\n",
    "print(f\"{'Modified':>10}: {format_timestamp(os.path.getmtime(outpath))}\")\n",
    "print(f\"{'Accessed':>10}: {format_timestamp(os.path.getatime(outpath))}\")\n",
    "print(f\"{'Now':>10}: {datetime.now(timezone.utc).isoformat(timespec='seconds')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dreambank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
